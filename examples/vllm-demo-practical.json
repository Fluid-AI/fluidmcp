{
  "llmModels": {
    "tinyllama": {
      "command": "python",
      "args": [
        "-m", "vllm.entrypoints.openai.api_server",
        "--model", "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        "--port", "8001",
        "--host", "0.0.0.0",
        "--max-model-len", "2048",
        "--dtype", "float16"
      ],
      "env": {},
      "endpoints": {
        "base_url": "http://localhost:8001/v1"
      }
    }
  }
}
