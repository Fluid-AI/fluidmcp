{
  "llmModels": {
    "vllm": {
      "model": "TEST_MODEL",
      "port": 8001,
      "profile": "production",
      "config": {
        "gpu_memory_utilization": 0.9,
        "max_num_seqs": 64,
        "max_num_batched_tokens": 8192,
        "max_model_len": 4096,
        "dtype": "float16",
        "tensor_parallel_size": 1
      },
      "timeouts": {
        "startup": 300,
        "streaming": null,
        "non_streaming": 120
      },
      "env": {}
    }
  }
}
